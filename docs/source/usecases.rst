Use Cases
=========

Anticipate security exploits 
----------------------------

*Suppose you were a developer assigned to deploy a new AI into the real world, 
or a parent or teacher preparing someone to enter society, or you were preparing 
yourself for a career, career shift, or to be a spouse or parent.* 
It would be malpractice not to take every reliable inexpensive opportunity to discern 
any exploits to which that AI or person would become vulnerable upon transition. 

Games are a time-tested method to develop or establish ability with logic and 
planning, and can also establish other kinds of ability, such as 
social and political skills, trend-spotting or setting, and innovation. The concept  
of "game" can be broad enough even to include parenting, since parents make moves, 
moves impact outcomes, and some outcomes are preferred over others. Real world 
parenting, stock trading, and warfare are too expensive and consequential to serve as 
training exercises, so maybe we say we "live" these games, rather than say we "play" 
them. However, because they *are* games, skills can help, and we should test 
our parenting, stock trading, and warfare skills *before* "living" those games. 

When the most comprehensive Olympics is sufficiently comprehensive and its most skilled 
players are sufficiently skilled, playing that Olympics against those players will test 
for the full range of vulnerabilities. Note that learning is not the only way that 
vulnerabilities can be overcome; sometimes skills are established via tools (e.g. 
some people establish vision via glasses) or via collaboration. The purpose of a 
security audit is to discern which tools, collaborators, or learning remain needed:

#. Have the human :doc:`establish an account <player>` (or, if testing AI, 
   :doc:`build it<playerfactory>` in redscience).
#. Identify the most comprehensive :doc:`Olympics <tournament>` via the 
   **Comparison Tab** and identify the reigning champions of its events via its 
   :doc:`leaderboard <game>`. Have the player under investigation play the Olympics 
   against its champions and the champions' "favorite" opponents. In cooperative games 
   (e.g. nuclear disarmament) the "exploiter" is likely to be someone who is difficult 
   to cooperate with (antisocial, dogmatic, unhelpfully biased), rather than a champion. 
   They will be a "favorite" of champions because the ability to deal with exploiters 
   sets champions apart.
#. Look at the player’s **Favoritism Tab** to identify vulnerabilities as 
   opponents and events for which the player under investigation consistently 
   performs worse than do other players of the same skill-level.
#. To understand a vulnerability, look at the favoritism stats of the expoiter to 
   find other victims who get exploited in the same way. Profile the victims as a group 
   to identify common traits (e.g. risk-aversion? risk-proclivity?)
#. To map the domain of "safety", browse the **Favoritism Tab** to find events in 
   which no vulnerability manifests. What do such events have in common (and what 
   real-world situations share that commonality)?
#. (For humans) to find ways to extend safety, have the player under investigation play 
   unsafe events using the top AI as :doc:`tools <playerfactory>`. Which forms of 
   tool use neutralize the handicap (i.e. review, debate, or delegation)? In other words, 
   how much agency can one preserve without also preserving the vulnerability? In the 
   real world, tools used in the same way as AI here include consultants/advisors, 
   juries, the algorithms encoded in scripture, law, and business processes (etc). 
   Experimenting with vulnerabilites in redscience may help us minimize loss of agency 
   to such tools.

.. Warning:: Patterns in the ways you can be defeated in various games 
  constitute private information (like personality test scores, 
  standardized test scores, or the results of genetic tests), so use 
  an account that cannot be traced to you whenever playing large numbers
  of games alone!
  
.. Note:: "Personality" settings are made available in redscience only if games
  have been identified for which different settings are optimal. Individual humans who 
  exhibit those traits would be vulnerable to some kind of exploit, but also have the 
  potential to strengthen a team by protecting it against other exploits (i.e. as cell 
  differentiation benefits a body).
  

Benchmark social designs
------------------------

*Suppose you were assembling a team of humans, or an AI, or a 
collaboration between humans and AI to play a real-world game, such as 
stocktrading, diplomacy, policymaking, to compete in business, or to 
address a problem or need.* One approach is to try to copy whatever design is 
currently most successful (e.g. poach from successful teams and ask the poached 
employees to replicate what worked for them in the past). That approach might
be called "dogma".

Dogma is sub-optimal if existing social designs are sub-optimal. Previous 
simulations find that existing social designs retard social progress 
`by 3 to 25 times <https://figshare.com/articles/dataset/Varieties_of_Elitism/7052264>`_. 
This should not surprise us, since we can look back in history to find social 
designs that are considerd barbaric today, even though they were the most 
successful of their age. 

An alternative approach--a way to escape dogma--is to test alternative 
social designs via simulations before deploying them in real-life. 
When the most :doc:`comprehensive Olympics <tournament>` in redscience is 
sufficiently comprehensive and its top non-human players are sufficiently 
skilled, the alternative approach will already have been completed for you 
and you would simply build real-world teams that match the team-sizes, personality 
ratios, curriculum ratios, and collaboration techniques of redscience champions. 

If scripture were a guide for social engineering, platforms like redscience 
would be its replacement, but, unlike scripture, redscience needn't identify 
with any particular religion and it offers those who question its wisdom a 
procedure to challenge that wisdom. For example, if the top non-human champion 
were a team that included an extreme personality that social engineers hesitated 
to include in real-world teams (e.g. as some social engineers have hestitated to 
include "feminine" personalities in certain leadership teams), the engineers 
could challenge the wisdom of including that personality as follows:    

#. :doc:`Clone <playerfactory>` the top team to create a new one, and make the 
   objectionable personality less extreme in the cloned member. 
#. Run an Olympic :doc:`tournament <tournament>` which includes both the 
   parent and its modified clone. Does the modified clone 
   outperform its parent? If not, are there specific events in which it does? 
   What real-world situations match the events on which the parent outperforms 
   the clone (i.e. what is there to appreciate about the personality)?

This is not a claim that science will instantly discern all wisdom 
and completely displace all other sources of wisdom; it is merely a 
claim that science can become useful to guide not only physical engineering 
and medicine but also to guide social engineering, and that platforms like 
redscience can make science as accessible as scripture. For example, if we 
previously turned to scripture to validate our approaches to personality 
differences, redscience will displace scripture for that function (something 
which scientific journals have not been efficient-enough to do).

.. Note:: The most comprehensive Olympics will include cooperative games 
  (like the *Public Goods game*), alliance games (like *Risk*), deception 
  games (like *Hide and Seek*), and probabilistic games (like *Poker*), 
  as well as planning games (like *Chess*), so this approach 
  hedges against the potential for any real-world game to 
  shift in any of these directions. If we can limit the shifting of real-world
  games, then it may be appropriate to use Olympics that are not the most 
  comprehensive in the procedures above.


Discover new dimensions of intelligence
---------------------------------------

*Suppose you loved someone so much that you wanted to leave a valuable 
legacy to their children and to the generations that follow. More than build an
empire that could be replaced, you want to advance the very standard of quality 
so that any replacement would build on your legacy.* What advance of quality 
could be more enriching than the introduction of a new dimension of intelligence (e.g. 
granting a culture its first awareness of empathy, tool-use, exploration 
or other not-yet-named dimension of intelligence)? 

Intelligence is measured in terms of the kinds of games which one being 
wins over another, so each dimension of intelligence can be expressed as a 
set of games (e.g. empathy can be expressed as games in which empathic 
players have advantage, perhaps because those games require collaboration
with players with different skill-level and norms). The most comprehensive 
:doc:`Olympics <tournament>` would test every dimension of intelligence, so the 
legacy of making the most comprehensive Olympics more comprehensive (while 
maintaining elementality) is like the legacy of expanding the Periodic Table of 
the Elements:

#. Identify the most comprehensive Olympics via the **Comparison Tab**
#. Use the **Comparison Tab** on the events of that Olympics to identify an 
   essential event in it, then fine-tune tools for that specific event (see 
   `Benchmark social designs`_). 
#. Contrast :doc:`the best tools for that event <game>` to the best tools 
   for other events to understand which :doc:`tools’ biases <playerfactory>` 
   are particularly advantageous for that event.
#. :doc:`Clone the event and tweak its design <gamefactory>` to make those 
   biases even more advantageous.
#. Use the **Comparison Tab** to confirm that swapping-in the new event makes 
   the Olympics more comprehensive.  

Elevate reality above experimentation
-------------------------------------

*Suppose our society were divided by competing systems of social norms.* For
example, the best strategy in the *Volunteer* game depends upon prevailing 
social norms which happen to correspond to the real-world norms of "turn-taking"
vs "caste system" (which sometimes manifests as racial discrimination). One way 
to resolve the real-world division might be to benchmark those norms in redscience: 

#. Copy the top-ranked *AI* for the *Volunteer* :doc:`game <game>` to a new 
   *Universe* (but :doc:`do not copy its curriculum <playerfactory>`). Play a 
   turn-taking strategy against it (i.e. “You volunteered last time, now it’s my 
   turn”) and confirm that it learns to take turns. Make several copies of that 
   *AI* in that *Universe*.
#. Similarly create a second private *Universe* in which you train all *AI* 
   to play *Volunteer* via caste (i.e. whoever got the better deal last time 
   gets it again). 
#. Copy an *AI* from the turn-taking *Universe* to the caste *Universe* (retaining
   its turn-taking experience), and confirm that it switches to the caste strategy. 
#. Copy an *AI* from the caste *Universe* to the turn-taking *Universe* (retaining 
   its caste experience) and confirm that it switches to turn-taking.
#. Create a third private *Universe* composed of equal numbers of players from the  
   first two *Universes*. Which norm survives a *Volunteer* :doc:`tournament <tournament>`?
   Similarly test other population ratios to find the minimum ratio for the 
   other norm to survive. 
#. Observe how freedom to select social situations impacts norms by running tournaments 
   where each reselection of players is composed of a player and their favorite 
   opponent. Repeat the experiment where each reselection is composed of two random 
   players plus the favorite opponent of the top-ranked player.

If we couldn’t run these experiments to our satisfaction in redscience, 
would we be doomed to spend our real lives serving as the subjects in 
such experiments (i.e. as pawns in a war between competing systems of 
norms)?

Empower students of social science and computer science
-------------------------------------------------------

*Suppose you were a social science teacher or computer science teacher*. It's one thing
to expose students to new ideas, but another thing to empower students to test 
those ideas for themselves. Although redscience is designed to be accessible at
the secondary-education level, it is just as relevant in post-secondary education.

* A social science teacher could assign students to `Benchmark social designs`_,
  `Anticipate security exploits`_, or `Elevate reality above experimentation`_

* A computer science teacher could assign students to `Anticipate security exploits`_
  (so they are aware of the security vulnerabilities of AI) and to 
  `Build their own redscience <curriculum>`_
