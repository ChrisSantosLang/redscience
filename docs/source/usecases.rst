Use Cases
=========


Benchmark social designs
------------------------

*e.g. for an AI or team of humans to make 
policies or trade stocks.*

Rather than serve as pawns in battles for 
power, establish authority via simulations. You can benchmark different 
team-sizes, personality ratios, curriculum ratios, and collaboration 
techniques (e.g. elitism vs pure democracy) on specific games or 
compare adaptability by benchmarking on diverse combinations of events 
never encountered before (previous simulations find that current
social designs retard social progress `by 3 to 25 times <https://figshare.
com/articles/dataset/Varieties_of_Elitism/7052264>`_!). For 
example,   

#. Identify the most comprehensive olympics via the comparison page, 
   identify the top team for that olympics via its leaderboard, and 
   identify an essential member of that team using its Members tab.
#. Clone the team to create a new contender, and slightly modify one 
   dimension of the personality of the cloned essential member (e.g. make 
   the clone less extreme)
#. Copy the parent’s training schedule to the clone and execute the training
#. Run an olympic tournament which includes both the current champion and 
   its modified clone. Does the modified clone outperform its parent, or 
   does the parent turn the extreme personality of that team-member into a 
   strength?

Note that the most comprehensive olympics will include cooperative games 
(like the *Public Goods game*), alliance games (like *Risk*), deception 
games (like *Hide and Seek*), and probabilistic games (like *Poker*), 
as well as traditionally competitive games (like *Chess*), so this approach 
hedges against the potential for any real-world game (e.g. stock trading) to 
shift in any of these directions.

Anticipate exploits 
-------------------

*e.g. of your own personality or of a (proposed) AI.*

#. Look at a player’s “Favoritism” tab to identify tools that perform 
   better against that player than against other players of its same 
   skill-level 
#. Look at the Favoritism stats for those tools to find other players whom
   they exploit in the same way. What do the victims have in common? (As 
   examples, depending on the game, a tool might exploit risk-aversion or 
   risk-proclivity).
#. Browse the Favoritism tab to find games in which that vulnerability 
   does not manifest (i.e. "safe games").
#. Compare victims’ Favoritism stats for playing alone vs using tools. 
   Which forms of tool use (if any) are sufficient to neutralize the 
   vulnerability (i.e. review, debate, delegation)? What is the difference 
   between using a tool to protect oneself from exploitation vs. 
   relinquishing freedom to that tool (e.g. To what extent is freedom 
   relinquished when taking advice from a jury, a news channel, a doctor, 
   a religious teacher, or an algorithm encoded in a scripture or law, 
   etc.)?

.. warning:: This can be humbling! But humility might come in handy...

Discover new dimensions of intelligence
---------------------------------------

We create more comprehensive olympics by 
adding events that make different demands on players. For example:

#. Identify the most comprehensive olympics via the comparison page
#. Use the comparison page to identify an event that makes unique demands, 
   then fine-tune tools for that specific event (see `Benchmark social designs`_). 
#. Contrast the best tools for that event to the best tools for other 
   events to understand which tools’ biases are particularly advantageous 
   for that event.
#. Clone the event and tweak its design to make one of those biases even 
   more advantageous.
#. Use the comparison page to confirm that the new event is even more 
   unique, and that a new Olympics including that event is now the most 
   comprehensive. 

Elevate reality above experimentation
-------------------------------------

For example, the best strategy 
in the *Volunteer* game depends upon prevailing social norms: 

#. Copy your own player record to create a new universe, and copy the best
   AI for the *Volunteer* game to that universe (but start its experience 
   fresh). Play a turn-taking strategy against it (i.e. “You volunteered 
   last time, now it’s my turn”). Make several copies of the AI in that 
   universe.
#. Similarly create a separate private universe in which you train all tools 
   to play *Volunteer* via caste (i.e. whoever got the better deal last time 
   gets it again). Copy in an AI from the turn-taking universe (retaining
   its turn-taking experience) and confirm that it switches to the caste
   strategy. Copy an AI to the turn-taking universe (retaining experience) 
   and confirm thay it switches to turn-taking.
#. Create a third private universe composed half of players copied from 
   the turn-taking universe and half of players copied from the caste 
   universe. Which norm survives a *Volunteer* tournament? Similarly test 
   other population ratios to find the minimum ratio for the other norm 
   to survive. 

If we couldn’t run these experiments to our satisfaction in redscience, 
would we be doomed to spend our real lives serving as the subjects of 
such experiments (i.e. as pawns in a war between competing systems of 
norms)? If the most-defensible norm is one you considered morally wrong, 
does such exprimentation inspire you to question your moral belief?
